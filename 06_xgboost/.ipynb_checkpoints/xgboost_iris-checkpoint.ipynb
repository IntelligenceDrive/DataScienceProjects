{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c4c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Jun 14 00:23:44 2021\n",
    "\n",
    "@author: G. Cao\n",
    "\"\"\"\n",
    "from sklearn import datasets\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8ef562",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc35fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Let’s get all of our data set up. We’ll start off by creating a train-test split so we can see just how well XGBoost performs. \n",
    "# # We’ll go with an 80%-20% split this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "342c4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d1ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order for XGBoost to be able to use our data, we’ll need to transform it into \n",
    "# a specific format that XGBoost can handle. That format is called DMatrix. It’s a \n",
    "# very simple one-linear to transform a numpy array of data to DMatrix format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae02297",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b651be",
   "metadata": {},
   "source": [
    "# Defining an XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d65c596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simplest parameters are the max_depth (maximum depth of the decision trees being trained), \n",
    "# objective (the loss function being used), and num_class (the number of classes in the dataset). \n",
    "# The eta ( the learning ratae) algorithm requires special attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d484fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 3,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "steps = 20  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a1e479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is common to have small values in the range of 0.1 to 0.3. \n",
    "# The smaller weighting of these residuals will still help us train a powerful model, \n",
    "# but won’t let that model run away into deep complexity where overfitting is more likely to happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "752f5cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:32:17] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a4fcd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5bf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(D_test)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8651f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 1.0\n",
      "Recall = 1.0\n",
      "Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision = {}\".format(precision_score(Y_test, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(Y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(Y_test, best_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a64ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     The gamma parameter can also help with controlling overfitting. It specifies the minimum reduction in the loss required to make a further partition on a leaf node of the tree. I.e if creating a new node doesn’t reduce the loss by a certain amount, then we won’t create it at all.\n",
    "#     The booster parameter allows you to set the type of model you will use when building the ensemble. The default is gbtree which builds an ensemble of decision trees. If your data isn’t too complicated, you can go with the faster and simpler gblinear option which builds an ensemble of linear models.\n",
    "#     Setting the optimal hyperparameters of any ML model can be a challenge. So why not let Scikit Learn do it for you? We can combine Scikit Learn’s grid search with an XGBoost classifier quite easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21a8575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8425a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "parameters = {\n",
    "     \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "     \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47c1ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(clf,\n",
    "                    parameters, n_jobs=4,\n",
    "                    scoring=\"neg_log_loss\",\n",
    "                    cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "047858a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21:44:33] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program_Files\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None, gamma=None,\n",
       "                                     gpu_id=None, importance_type='gain',\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_delta_step=None,\n",
       "                                     max_depth=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs...\n",
       "                                     num_parallel_tree=None, random_state=None,\n",
       "                                     reg_alpha=None, reg_lambda=None,\n",
       "                                     scale_pos_weight=None, subsample=None,\n",
       "                                     tree_method=None, validate_parameters=None,\n",
       "                                     verbosity=None),\n",
       "             n_jobs=4,\n",
       "             param_grid={'colsample_bytree': [0.3, 0.4, 0.5, 0.7],\n",
       "                         'eta': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3],\n",
       "                         'gamma': [0.0, 0.1, 0.2, 0.3, 0.4],\n",
       "                         'max_depth': [3, 4, 5, 6, 8, 10, 12, 15],\n",
       "                         'min_child_weight': [1, 3, 5, 7]},\n",
       "             scoring='neg_log_loss')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04c3583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once your XGBoost model is trained, you can dump a human readable description of it into a text file:\n",
    "model.dump_model('dump.raw.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda5029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
